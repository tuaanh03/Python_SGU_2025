{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOcriKx3LrqpNlODFU0g31O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CGd09a8R7bCt","executionInfo":{"status":"ok","timestamp":1764257366397,"user_tz":-420,"elapsed":19119,"user":{"displayName":"Anh Ngo","userId":"02646037703594249256"}},"outputId":"cd7d8168-c9f8-47a9-e206-cebcbc4f55c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting Faker\n","  Downloading faker-38.2.0-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from Faker) (2025.2)\n","Downloading faker-38.2.0-py3-none-any.whl (2.0 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Faker\n","Successfully installed Faker-38.2.0\n","--- Đã tạo 10000 mẫu ---\n","Ví dụ: 1977-06-30 => 30/06/1977\n"]}],"source":["!pip install Faker\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F # Cần cho Attention\n","import random\n","import numpy as np\n","from faker import Faker\n","\n","# --- 1. Chuẩn bị Dữ liệu ---\n","fake = Faker()\n","\n","vocab = list(\"0123456789-,/ ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\")\n","vocab = ['<PAD>', '<SOS>', '<EOS>'] + vocab\n","char_to_index = {char: i for i, char in enumerate(vocab)}\n","index_to_char = {i: char for i, char in enumerate(vocab)}\n","\n","VOCAB_SIZE = len(vocab)\n","MAX_LENGTH = 30 # Độ dài tối đa\n","\n","def generate_data(num_samples=10000): # Dùng 10k mẫu\n","    data = []\n","    formats = ['%B %d, %Y', '%Y-%m-%d', '%A, %b %d %Y', '%d-%m-%Y']\n","    output_format = '%d/%m/%Y'\n","    for _ in range(num_samples):\n","        d = fake.date_object()\n","        input_str = d.strftime(random.choice(formats))\n","        target_str = d.strftime(output_format)\n","        data.append((input_str[:MAX_LENGTH-2], target_str[:MAX_LENGTH-2]))\n","    return data\n","\n","def string_to_tensor(s):\n","    tensor = [char_to_index['<SOS>']]\n","    tensor += [char_to_index.get(c, 0) for c in s]\n","    tensor.append(char_to_index['<EOS>'])\n","\n","    # Padding\n","    while len(tensor) < MAX_LENGTH:\n","        tensor.append(char_to_index['<PAD>'])\n","\n","    return torch.tensor(tensor, dtype=torch.long).view(-1, 1)\n","\n","# Tạo mẫu\n","train_data = generate_data(10000) # 10k mẫu là đủ\n","print(f\"--- Đã tạo {len(train_data)} mẫu ---\")\n","print(f\"Ví dụ: {train_data[0][0]} => {train_data[0][1]}\")"]},{"cell_type":"code","source":["# --- 1. ENCODER (Sửa đổi) ---\n","# Bây giờ trả về TẤT CẢ outputs, và (hidden, cell)\n","class EncoderLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.lstm = nn.LSTM(hidden_size, hidden_size)\n","\n","    def forward(self, input_tensor):\n","        embedded = self.embedding(input_tensor)\n","        # outputs shape: [seq_len, 1, hidden_size]\n","        # hidden/cell shape: [1, 1, hidden_size]\n","        outputs, (hidden, cell) = self.lstm(embedded)\n","        return outputs, hidden, cell # TRẢ VỀ TẤT CẢ OUTPUTS\n","\n","# --- 2. ATTENTION (Module MỚI) ---\n","# Đây là \"bộ não\" của sự chú ý (Additive Attention)\n","class Attention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention, self).__init__()\n","        # Lớp Linear để kết hợp hidden_state của Decoder và Encoder\n","        self.Wa = nn.Linear(hidden_size * 2, hidden_size) # (decoder_hidden + encoder_output)\n","        self.Va = nn.Linear(hidden_size, 1) # Lấy 1 điểm số duy nhất\n","\n","    def forward(self, decoder_hidden, encoder_outputs):\n","        # encoder_outputs shape: [seq_len, hidden_size]\n","        # decoder_hidden shape: [1, hidden_size]\n","\n","        seq_len = encoder_outputs.shape[0]\n","\n","        # Lặp lại decoder_hidden để nó có [seq_len, hidden_size]\n","        decoder_hidden_expanded = decoder_hidden.repeat(seq_len, 1)\n","\n","        # Nối (concatenate)\n","        # [seq_len, hidden_size*2]\n","        concat_hidden = torch.cat((decoder_hidden_expanded, encoder_outputs), dim=1)\n","\n","        # [seq_len, hidden_size]\n","        energy = torch.tanh(self.Wa(concat_hidden))\n","\n","        # [seq_len, 1]\n","        attention_scores = self.Va(energy)\n","\n","        # [seq_len, 1] -> [1, seq_len]\n","        # Dùng softmax để chuẩn hóa, cho biết nên \"chú ý\" vào đâu\n","        weights = F.softmax(attention_scores.T, dim=1)\n","\n","        # [1, seq_len] @ [seq_len, hidden_size] -> [1, hidden_size]\n","        # Tính \"context vector\" (ý niệm) dựa trên sự chú ý\n","        context_vector = torch.bmm(weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n","\n","        return context_vector.squeeze(0), weights.squeeze(0) # Trả về context và trọng số (để debug)\n","\n","# --- 3. DECODER (Viết lại hoàn toàn) ---\n","# Bây giờ gọi là AttnDecoderLSTM\n","class AttnDecoderLSTM(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(AttnDecoderLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.attention = Attention(hidden_size) # Nhúng Attention vào\n","\n","        # LSTM bây giờ nhận (input + context_vector)\n","        self.lstm = nn.LSTM(hidden_size * 2, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input_char, decoder_hidden, decoder_cell, encoder_outputs):\n","        # input_char shape: [1, 1]\n","        embedded = self.embedding(input_char).view(1, 1, -1)\n","\n","        # Lấy \"ý niệm\" từ Attention\n","        # decoder_hidden[0] vì shape của nó là [1, 1, H]\n","        context_vector, attn_weights = self.attention(decoder_hidden[0], encoder_outputs)\n","\n","        # Nối (concatenate) input (embedded) và \"ý niệm\" (context)\n","        # [1, 1, H] + [1, 1, H] -> [1, 1, 2*H]\n","        lstm_input = torch.cat((embedded, context_vector.unsqueeze(0)), dim=2)\n","\n","        # Cho qua LSTM\n","        output, (hidden, cell) = self.lstm(lstm_input, (decoder_hidden, decoder_cell))\n","\n","        # Dự đoán\n","        prediction = self.softmax(self.out(output[0]))\n","        return prediction, hidden, cell, attn_weights # Trả về cả attn_weights"],"metadata":{"id":"a0Dp6tDd77vg","executionInfo":{"status":"ok","timestamp":1764257417282,"user_tz":-420,"elapsed":33,"user":{"displayName":"Anh Ngo","userId":"02646037703594249256"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# --- 1. Định nghĩa tham số ---\n","hidden_size = 128 # Tăng lên cho mô hình mạnh hơn\n","learning_rate = 0.001 # Giảm LR đi một chút\n","n_epochs = 10000 # 10k epochs (mỗi epoch 1 mẫu)\n","log_every = 500\n","teacher_forcing_ratio = 0.75 # Tăng teacher forcing\n","\n","# --- 2. Khởi tạo mô hình ---\n","encoder = EncoderLSTM(VOCAB_SIZE, hidden_size)\n","decoder = AttnDecoderLSTM(hidden_size, VOCAB_SIZE) # Dùng AttnDecoder\n","\n","encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n","decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n","criterion = nn.NLLLoss(ignore_index=char_to_index['<PAD>'])\n","\n","# --- 3. Vòng lặp Huấn luyện ---\n","print(\"--- Bắt đầu huấn luyện (với ATTENTION)... ---\")\n","total_loss_avg = 0\n","\n","for epoch in range(1, n_epochs + 1):\n","\n","    input_str, target_str = random.choice(train_data)\n","    input_tensor = string_to_tensor(input_str)\n","    target_tensor = string_to_tensor(target_str)\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","    loss = 0\n","\n","    # --- ENCODER ---\n","    # Chạy Encoder và lấy TẤT CẢ outputs\n","    encoder_outputs, encoder_hidden, encoder_cell = encoder(input_tensor)\n","\n","    # Bỏ đi shape [seq_len, 1, H] -> [seq_len, H]\n","    encoder_outputs_squeezed = encoder_outputs.squeeze(1)\n","\n","    # --- DECODER ---\n","    decoder_input = torch.tensor([[char_to_index['<SOS>']]], dtype=torch.long)\n","    decoder_hidden = encoder_hidden # Dùng \"ý niệm\" cuối\n","    decoder_cell = encoder_cell\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","    target_len = 10 # Chỉ dịch 10 ký tự (dd/mm/yyyy)\n","\n","    if use_teacher_forcing:\n","        for i in range(target_len):\n","            decoder_output, decoder_hidden, decoder_cell, _ = decoder(\n","                decoder_input, decoder_hidden, decoder_cell, encoder_outputs_squeezed\n","            )\n","            loss += criterion(decoder_output, target_tensor[i+1])\n","            decoder_input = target_tensor[i+1].view(1, 1)\n","    else:\n","        for i in range(target_len):\n","            decoder_output, decoder_hidden, decoder_cell, _ = decoder(\n","                decoder_input, decoder_hidden, decoder_cell, encoder_outputs_squeezed\n","            )\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach().view(1, 1)\n","\n","            loss += criterion(decoder_output, target_tensor[i+1])\n","            if decoder_input.item() == char_to_index['<EOS>']:\n","                break\n","\n","    loss.backward()\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","    total_loss_avg += loss.item() / target_len\n","\n","    if epoch % log_every == 0:\n","        print(f\"Epoch {epoch}/{n_epochs}, Avg Loss: {total_loss_avg / log_every:.4f}\")\n","        total_loss_avg = 0\n","\n","print(\"--- Huấn luyện thủ công hoàn tất ---\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Pk0MfXt8vlo","executionInfo":{"status":"ok","timestamp":1764258018932,"user_tz":-420,"elapsed":389326,"user":{"displayName":"Anh Ngo","userId":"02646037703594249256"}},"outputId":"840baf98-ad85-4cce-a6e2-120c44d1def5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Bắt đầu huấn luyện (với ATTENTION)... ---\n","Epoch 500/10000, Avg Loss: 1.5476\n","Epoch 1000/10000, Avg Loss: 1.0444\n","Epoch 1500/10000, Avg Loss: 0.6630\n","Epoch 2000/10000, Avg Loss: 0.1414\n","Epoch 2500/10000, Avg Loss: 0.0797\n","Epoch 3000/10000, Avg Loss: 0.0209\n","Epoch 3500/10000, Avg Loss: 0.0124\n","Epoch 4000/10000, Avg Loss: 0.0205\n","Epoch 4500/10000, Avg Loss: 0.0034\n","Epoch 5000/10000, Avg Loss: 0.0020\n","Epoch 5500/10000, Avg Loss: 0.0014\n","Epoch 6000/10000, Avg Loss: 0.0523\n","Epoch 6500/10000, Avg Loss: 0.0099\n","Epoch 7000/10000, Avg Loss: 0.0067\n","Epoch 7500/10000, Avg Loss: 0.0100\n","Epoch 8000/10000, Avg Loss: 0.0224\n","Epoch 8500/10000, Avg Loss: 0.0020\n","Epoch 9000/10000, Avg Loss: 0.0033\n","Epoch 9500/10000, Avg Loss: 0.0008\n","Epoch 10000/10000, Avg Loss: 0.0004\n","--- Huấn luyện thủ công hoàn tất ---\n"]}]},{"cell_type":"code","source":["def predict_manual_with_attention(input_str):\n","    print(f\"\\nInput: {input_str}\")\n","    input_tensor = string_to_tensor(input_str)\n","\n","    with torch.no_grad():\n","        # ENCODER\n","        encoder_outputs, encoder_hidden, encoder_cell = encoder(input_tensor)\n","        encoder_outputs_squeezed = encoder_outputs.squeeze(1)\n","\n","        # DECODER\n","        decoder_input = torch.tensor([[char_to_index['<SOS>']]], dtype=torch.long)\n","        decoder_hidden = encoder_hidden\n","        decoder_cell = encoder_cell\n","\n","        output_string = \"\"\n","\n","        for _ in range(10): # Chỉ sinh 10 ký tự\n","            decoder_output, decoder_hidden, decoder_cell, attn_weights = decoder(\n","                decoder_input, decoder_hidden, decoder_cell, encoder_outputs_squeezed\n","            )\n","\n","            topv, topi = decoder_output.topk(1)\n","            char_index = topi.squeeze().item()\n","\n","            if char_index == char_to_index['<EOS>']:\n","                break\n","\n","            output_string += index_to_char[char_index]\n","            decoder_input = topi.squeeze().view(1, 1)\n","\n","    print(f\"Output: {output_string}\")\n","\n","# Chạy thử\n","predict_manual_with_attention(\"July 4, 1990\")\n","predict_manual_with_attention(\"2024-12-25\")\n","predict_manual_with_attention(\"Saturday, Sep 20 2025\")\n","predict_manual_with_attention(fake.date_object().strftime(random.choice(['%B %d, %Y', '%Y-%m-%d'])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxGceCdi-wvI","executionInfo":{"status":"ok","timestamp":1764258158327,"user_tz":-420,"elapsed":61,"user":{"displayName":"Anh Ngo","userId":"02646037703594249256"}},"outputId":"ab7e8fd9-c409-49f1-c6f8-53fe19fd2b1f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Input: July 4, 1990\n","Output: 19/07/1990\n","\n","Input: 2024-12-25\n","Output: 25/12/2024\n","\n","Input: Saturday, Sep 20 2025\n","Output: 20/09/2025\n","\n","Input: November 03, 2008\n","Output: 03/11/2008\n"]}]}]}